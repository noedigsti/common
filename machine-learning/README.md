# Basics

### algorithms

[TheAlgorithms/Python: All Algorithms implemented in Python](https://github.com/TheAlgorithms/Python)

### nn

[karpathy/nn-zero-to-hero: Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero)

### pytorch

[mrdbourke/pytorch-deep-learning: Materials for the Learn PyTorch for Deep Learning: Zero to Mastery course.](https://github.com/mrdbourke/pytorch-deep-learning)

### tensorflow

[mrdbourke/tensorflow-deep-learning: All course materials for the Zero to Mastery Deep Learning with TensorFlow course.](https://github.com/mrdbourke/tensorflow-deep-learning)

[Enzofali/The-Ultimate-Google-TensorFlow-Developer-Certificate-Exam-Study-Guide: Google TensorFlow Developer Certificate Study Guide](https://github.com/Enzofali/The-Ultimate-Google-TensorFlow-Developer-Certificate-Exam-Study-Guide)

# Model Architecture

### CNN

[philipperemy/keras-tcn: Keras Temporal Convolutional Network.](https://github.com/philipperemy/keras-tcn)

[bentrevett/pytorch-image-classification: Tutorials on how to implement a few key architectures for image classification using PyTorch and TorchVision.](https://github.com/bentrevett/pytorch-image-classification)

### RNN

[DataForScience/RNN: Recurrent Neural Networks for Timeseries](https://github.com/DataForScience/RNN)

[Understanding LSTM Networks -- colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/#lstm-networks)

[nicklashansen/rnn_lstm_from_scratch: How to build RNNs and LSTMs from scratch with NumPy.](https://github.com/nicklashansen/rnn_lstm_from_scratch)

### Seq2Seq

[bentrevett/pytorch-seq2seq: Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText.](https://github.com/bentrevett/pytorch-seq2seq)

### Transformer

[karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs.](https://github.com/karpathy/nanoGPT)

### Mixed

[bentrevett/pytorch-sentiment-analysis: Tutorials on getting started with PyTorch and TorchText for sentiment analysis.](https://github.com/bentrevett/pytorch-sentiment-analysis)

[afshinea/stanford-cs-230-deep-learning: VIP cheatsheets for Stanford's CS 230 Deep Learning](https://github.com/afshinea/stanford-cs-230-deep-learning)

[Kulbear/deep-learning-coursera: Deep Learning Specialization by Andrew Ng on Coursera.](https://github.com/Kulbear/deep-learning-coursera)

[huggingface/pytorch-image-models: PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, EfficientNetV2, NFNet, Vision Transformer, MixNet, MobileNet-V3/V2, RegNet, DPN, CSPNet, and more](https://github.com/huggingface/pytorch-image-models)

[DeepLearningDTU/02456-deep-learning-with-PyTorch: Exercises and supplementary material for the deep learning course 02456 using PyTorch.](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch)
# Reinforcement Learning

### RL

[tensorflow/agents: TF-Agents: A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.](https://github.com/tensorflow/agents)

[openai/baselines: OpenAI Baselines: high-quality implementations of reinforcement learning algorithms](https://github.com/openai/baselines)

[deepmind/alphastar](https://github.com/deepmind/alphastar)

[huggingface/deep-rl-class: This repo contains the syllabus of the Hugging Face Deep Reinforcement Learning Course.](https://github.com/huggingface/deep-rl-class)

[kimbring2/MOBA_RL: Deep Reinforcement Learning for Multiplayer Online Battle Arena](https://github.com/kimbring2/MOBA_RL)

[kimbring2/minecraft_ai](https://github.com/kimbring2/minecraft_ai)

### PPO

[llSourcell/OpenAI_Five_vs_Dota2_Explained: This is the code for "OpenAI Five vs DOTA 2 Explained" By Siraj Raval on Youtube](https://github.com/llSourcell/OpenAI_Five_vs_Dota2_Explained)

[vwxyzjn/ppo-implementation-details: The source code for the blog post The 37 Implementation Details of Proximal Policy Optimization](https://github.com/vwxyzjn/ppo-implementation-details)

# Training

[NVIDIA/Megatron-LM: Ongoing research training transformer models at scale](https://github.com/NVIDIA/Megatron-LM#collecting-gpt-webtext-data)
