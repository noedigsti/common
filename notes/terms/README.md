`Perceptron`: Basic artificial neuron used in machine learning. Learns from input data, computes weighted sum, applies activation function, and adjusts weights to improve predictions. Forms building block for neural networks.

`Linear regression`: Statistical technique to model the relationship between variables by finding the best-fitting line. Estimates coefficients that minimize the difference between predicted and actual values. Used for prediction and analysis.

`Gradient descent`: Optimization algorithm for minimizing a function by iteratively adjusting parameters in the direction of steepest descent. Used in machine learning and neural network training.

`Logistic regression`: is a _binary classification_ algorithm that models the probability of belonging to a class based on input features using a sigmoid function. It finds the optimal weights through training and assigns class labels based on a threshold.

`Locally weighted regression (LWR)`: is a non-parametric regression algorithm that predicts by considering nearby data points with varying weights based on their proximity. It adapts to local data characteristics and captures complex relationships.

`Stochastic gradient descent (SGD)`: Optimization algorithm that computes gradients on randomly selected mini-batches of data, allowing for efficient and faster convergence in machine learning models.

`Affine transformation`: is a geometric mapping that preserves parallel lines and ratios of distances through a combination of scaling, rotation, shearing, and translation operations.